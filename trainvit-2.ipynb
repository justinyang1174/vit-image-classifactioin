{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb1f08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed509186",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/Users/yangyaofu/Downloads/image/train'\n",
    "test_dir = '/Users/yangyaofu/Downloads/image/vaildation'\n",
    "\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  \n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  \n",
    "  class_names = train_data.classes\n",
    "\n",
    "  \n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2838b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 320\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])           \n",
    "#print(f\"Manually created transforms: {manual_transforms}\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names\n",
    "\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "\n",
    "print(image.shape, label)\n",
    " \n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);\n",
    "\n",
    "class PatchEmbedding(nn.Module):  \n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=8,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=2, \n",
    "                                  end_dim=3)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        \n",
    "        return x_flattened.permute(0, 2, 1)\n",
    "\n",
    "patch_size =8\n",
    "\n",
    "\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "set_seeds()\n",
    "\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=8,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patch_embedded_image) \n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "patch_size = 8\n",
    "\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True) \n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n",
    "\n",
    "\n",
    "print(patch_embedding_class_token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593722b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, \n",
    "                 num_heads:int=12, \n",
    "                 attn_dropout:float=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, \n",
    "                                             key=x, \n",
    "                                             value=x, \n",
    "                                             need_weights=False) \n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "    \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    # 5. Create a forward() method  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x \n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x\n",
    "\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "from torchinfo import summary\n",
    "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "       row_settings=[\"var_names\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=320, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=8, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        \n",
    "        # 3. Make the image size is divisble by the patch size \n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        \n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1) \n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "\n",
    "        return x      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff060fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from going_modular.going_modular import engine,utils\n",
    "import torch\n",
    "from transformers import TFAutoModelForImageClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import json\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \n",
    "optimizer = torch.optim.Adam(params=vit.parameters(), \n",
    "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
    "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
    "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=20,\n",
    "                       device=device)\n",
    "\n",
    "\n",
    "#def save_model_and_metadata(model, target_dir, model_name, metadata, metadata_name):\n",
    "#    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "#    torch.save(model.state_dict(), os.path.join(target_dir, model_name))\n",
    "    \n",
    "#    with open(os.path.join(target_dir, metadata_name), 'w', encoding='utf-8') as f:\n",
    "#        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#metadata = {\n",
    "#    \"model_name\": \"ViT\",\n",
    "#    \"epochs\": 20,\n",
    "#    \"optimizer\": \"Adam\",\n",
    "#    \"loss_fn\": \"CrossEntropyLoss\",\n",
    "#    \"accuracy\": 0.95  \n",
    "#}\n",
    "\n",
    "#save_model_and_metadata(\n",
    "#    model=vit,\n",
    "#    target_dir=\"going_modular\",\n",
    "#    model_name=\"cc.pth\",\n",
    "#    metadata=metadata,\n",
    "#    metadata_name=\"metadata.json\"\n",
    "#)\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "#vit.eval() 驗證  \n",
    "\n",
    "with open('results2.txt', 'w') as f:\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = vit(inputs)  # Forward pass\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "            # 將數據寫入文件\n",
    "            f.write(f\"Inputs: {inputs.cpu().numpy()}\\n\")\n",
    "            f.write(f\"Labels: {labels.cpu().numpy()}\\n\")\n",
    "            f.write(f\"Outputs: {outputs.cpu().numpy()}\\n\")\n",
    "            f.write(f\"Predictions: {preds.cpu().numpy()}\\n\")\n",
    "            f.write(\"\\n\")  \n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "\n",
    "#ROC\n",
    "true_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "vit.eval()  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = vit(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)  \n",
    "        predicted_prob = probs[:, 1].cpu().numpy()  \n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_probs.extend(predicted_prob)\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(true_labels, predicted_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddab826",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaderfrom helper_functions import plot_loss_curves\n",
    "\n",
    "\n",
    "#keras.models.save_model(result,\"modell.h5\")\n",
    "\n",
    "# Plot our ViT model's loss curves\n",
    "#plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15656f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Import function to make predictions on images and plot them \n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = \"/Users/yangyaofu/Downloads/image/vaildation/no/no.395.jpg\"\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e5ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48d28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e77e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
